# Coursera DeepLearning.ai
Coursera Deep Learning Specialization(by Andrew Ng) 강의 정리


## Course 1. Neural Networks and Deep Learning

Week2: [Neural Networks Basics](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/1.%20Neural%20Networks%20and%20Deep%20Learning/week2)
* Logistic Regression(로지스틱 회귀)
* Logistic Regression Cost Function(로지스틱 회귀 비용함수)
* Gradient Descent(경사하강법)
* Computation Graph & 미분
* Logistic Regression에서의 Gradient Descent


Week3: [Shallow Neural Networks](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/1.%20Neural%20Networks%20and%20Deep%20Learning/week3)
* Neural network Representation and Vectorization
* Activation Function(활성화 함수)
* non-linear Activation Function를 사용하는 이유
* Activation Function(활성화 함수) 미분  
* Weight Random Initialization(가중치 초기화)


Week4: [Deep Neural Networks](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/1.%20Neural%20Networks%20and%20Deep%20Learning/week4)
* Forward Propagation  
* Getting your Matrix dimension right  
* Why deep Representations? (깊은 신경망이 더 많은 특징을 잡아내는 이유, 직관적으로)  
* Parameters vs HyperParameters  
* Forward and Backward propagation  
  
  
  
## Course 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

Week1: [Practical aspects of Deep Learning](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/2.%20Improving%20Deep%20Neural%20Networks%20(Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization)/week1)
* Train/Dev/Test sets
* Bias/Variance
* Basic Recipe for Machine Learning
* Regularization(L1,L2)
* Why Regularization reduces overfitting?
* Dropout Regularization
* Understanding Dropout
* Other regularization methods
* Normalizing Inputs
* Vanishing/Exploding Gradients
* Weight Initialize for Deep Networks
* Numerical Approximations of Gradients
* Gradient Checking

  
Week2: [Optimization algorithms](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/2.%20Improving%20Deep%20Neural%20Networks%20(Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization)/week2)
* Mini Batch Gradient Descent
* Understanding Mini Batch Gradient Descent
* Exponentially Weight Average
* Bias Correction of Exponentially Weight Average
* Gradient Descent with Momentum
* RMSprop
* Adam Optimization Algorithm
* Learning Rate Decay
* The problem of Local Optima
  
  
Week3: [Hyperparameter tuning, Batch Normalization](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/2.%20Improving%20Deep%20Neural%20Networks%20(Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization)/week3)  
* Tuning Process
* Using an Appropriate Scale
* Normalizing Activations in a Network
* Fitting Batch Norm into Neural Networks
* Why Does Batch Norm Work?
* Batch Norm at Test Time
* Softmax Regression
* Training Softmax Classifier


## Course 3. Structuring Machine Learning Project

Week1: [ML Strategy(1)](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/3.%20Structuring%20Machine%20Learning%20Projects/week1)
* Orthonalization
* Single Number Evaluation Metric
* Satisficing and Optimizing Metrics
* Train/Dev/Test Set Distribusions
* Size of Dev/Test Sets
* When to change Dev/Test sets and Metrics?
* Why Human-Level Performance?
* Avoidable Bias(회피 가능 편향)
* Surpassing Human-Level Performance
* Improving Model Performance


Week2: [ML Strategy(2)](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/3.%20Structuring%20Machine%20Learning%20Projects/week2)
* Carrying Out Error Analysis
* Cleaning Up Incorrectly Labelled Data
* Training and Testing on Different Distribusions
* Bias and Variance with Mismatched Data Distribusions
* Addressing Data Mismatch
* Transfer Learning
* Multi-task Learning
* What is end-to-end Deep Learning
* Whether to use end-to-end Deep Learning



## Course4. Convolution Neural Networks

Week1: [Foundations of Convolutional Neural Networks](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/4.%20Convolution%20Neural%20Networks/week1)
* Computer Vision
* Edge Detection Examples
* More Edge Detection
* Padding
* Stride
* Convolutions Over Volume
* One layer of a Convolutional Net
* Simple Convolutional Networks Example
* Pooling Layers
* Why Convolutions?

Week2: [Deep Convolutional models(Case Studies)](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/4.%20Convolution%20Neural%20Networks/week2)
* Why Look at Case Studies?
* Classic Networks (LeNet, AlexNet, VGG)
* ResNets
* Why ResNets Work?
* Networks in Networks and 1x1 Convolutions
* Inception Network Motivation
* Inception Network
* Transfer Learning
* Data Augmentation
* State of Computer Vision

