# Coursera DeepLearning.ai
Coursera Deep Learning Specialization(by Andrew Ng) 강의 정리


## Course 1. Neural Networks and Deep Learning

Week2: [Neural Networks Basics](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/1.%20Neural%20Networks%20and%20Deep%20Learning/week2)
* Logistic Regression(로지스틱 회귀)
* Logistic Regression Cost Function(로지스틱 회귀 비용함수)
* Gradient Descent(경사하강법)
* Computation Graph & 미분
* Logistic Regression에서의 Gradient Descent


Week3: [Shallow Neural Networks](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/1.%20Neural%20Networks%20and%20Deep%20Learning/week3)
* Neural network Representation and Vectorization
* Activation Function(활성화 함수)
* non-linear Activation Function를 사용하는 이유
* Activation Function(활성화 함수) 미분  
* Weight Random Initialization(가중치 초기화)


Week4: [Deep Neural Networks](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/1.%20Neural%20Networks%20and%20Deep%20Learning/week4)
* Forward Propagation  
* Getting your Matrix dimension right  
* Why deep Representations? (깊은 신경망이 더 많은 특징을 잡아내는 이유, 직관적으로)  
* Parameters vs HyperParameters  
* Forward and Backward propagation  
  
  
  
## Course 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

Week1: [Practical aspects of Deep Learning](https://github.com/wooseok-shin/DeepLearning.ai/tree/master/2.%20Improving%20Deep%20Neural%20Networks%20(Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization)/week1)
* Train/Dev/Test sets
* Bias/Variance
* Basic Recipe for Machine Learing
* Regularization(L1,L2)
* Why Regularization reduces overfitting?
* Dropout Regularization
* Understanding Dropout
* Other regularization methods
  
* Normalizing Inputs
* Vanishing/Exploding Gradients
* Weight Initialize for Deep Networks
* Numerical Approximations of Gradients
* Gradient Checking